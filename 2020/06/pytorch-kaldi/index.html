<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Baileyswu">





<title>PyTorch-Kaldi简介 | Ugly Gardon</title>



    <link rel="icon" href="/hollow.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
    src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 5.3.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Ugly Garden</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/friends">Friends</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Ugly Garden</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/friends">Friends</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">PyTorch-Kaldi简介</h1>
            <section class="post-tags">
                <div>
                    <span class="tag">
                        
                        
                            <a class="iconfont icon-tags" href="/tag/PyTorch/">PyTorch</a>
                        
                            <a class="iconfont icon-tags" href="/tag/Kaldi/">Kaldi</a>
                        
                            
                    </span>
                </div>
                
                    <div>
                        <span id="/2020/06/pytorch-kaldi/" class="leancloud_visitors" data-flag-title="PyTorch-Kaldi简介">
                            <i class="leancloud-visitors-count">101</i>
                            <em class="post-meta-item-text"> views </em>
                        </span>
                    </div>
                
            </section>

            
                <div class="post-meta">
                    
                        
                            <a itemprop="author" rel="author" href="/" class="iconfont icon-resume">lili</a>
                        
                    

                    
                        <span class="post-category">
                            
                                <a class="iconfont icon-category" href="/category/%E6%8A%80%E6%9C%AF/">技术</a>
                            
                        </span>
                    
                    
                        <span class="post-time">
                        <a href="#">06/08, 2020</a>
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>网上关于 PyTorch-Kaldi 的介绍多停留在论文翻译，很难得在李理的博客里看到例子的运行和脚本的详细说明，以下附上原文内容并附上少量注解。</p>
<p><strong>本文转载自 <a target="_blank" rel="noopener" href="http://fancyerii.github.io/books/pytorch-kaldi/">李理的博客</a></strong></p>
<p>本文介绍 PyTorch-Kaldi。前面介绍过的 Kaldi 是用 C++ 和各种脚本来实现的，它不是一个通用的深度学习框架。如果要使用神经网络来梯度 GMM 的声学模型，就得自己用 C++ 代码实现神经网络的训练与预测，这显然很难实现并且容易出错。我们更加习惯使用 Tensorflow 或者 PyTorch 来实现神经网络。因此 PyTorch-Kaldi 就应运而生了，它使得我们可以利用 Kaldi 高效的特征提取、HMM 模型和基于 WFST 的解码器，同时使用我们熟悉的 PyTorch 来解决神经网络的训练和预测问题。阅读本文前需要理解 HMM-DNN 的语音识别系统、WFST和Kaldi的基本用法。</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>前面我们了解了 Kaldi 的基本用法，Kaldi 最早设计是基于 HMM-GMM 架构的，后来通过引入 DNN 得到 HMM-DNN 模型。但是由于 Kaldi 并不是一个深度学习框架，我们如果想使用更加复杂的深度学习算法会很困难，我们需要修改 Kaldi 里的 C++ 代码，需要非常熟悉其代码才能实现。而且我们可能需要自己实现梯度计算，因为它不是一个 Tensorflow 或者 PyTorch 这样的框架。这样就导致想在Kaldi里尝试不同的深度学习(声学)模型非常困难。而 PyTorch-Kaldi 就是为了解决这个问题，它的架构如图<a href='#pykaldi'>下图</a>所示，它把 PyTorch 和 Kaldi 完美的结合起来，使得我们可以把精力放到怎么用 PyTorch 实现不同的声学模型，而把 PyTorch 声学模型和Kaldi复杂处理流程结合的 dirty 工作它都帮我们做好了。<br> <a name='pykaldi'><img src="/2020/06/pytorch-kaldi/py-kaldi.png" class=""></a><br><em>图：PyTorch-Kaldi架构</em></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><a target="_blank" rel="noopener" href="https://github.com/mravanelli/pytorch-kaldi">PyTorch-Kaldi</a> 的目的是作为 Kaldi 和 PyTorch 的一个<strong>桥梁</strong>，它能继承 Kaldi 的高效和 PyTorch 的灵活性。PyTorch-Kaldi 并不只是这两个工具的粘合剂，而且它还提供了用于构建现代语音识别系统的很多有用特性。比如，代码可以很容易的插入用户自定义的声学模型。此外，用户也可以利用预先实现的网络结果，通过简单的配置文件修改就可以实现不同的模型。PyTorch-Kaldi 也支持多个特征 (feature) 和标签 (label) 流的<strong>融合</strong>，使用复杂的网络结构。 它提供完善的文档并且可以在本地或者HPC集群上运行。</p>
<p>下面是最新版本的一些特性：</p>
<ul>
<li>使用 Kaldi 的简单接口</li>
<li>容易插入 (plug-in) 自定义模型</li>
<li>预置许多常见模型，包括 MLP, CNN, RNN, LSTM, GRU, Li-GRU, SincNet</li>
<li>基于多种特征、标签和网络结构的复杂模型实现起来非常自然。</li>
<li>简单和灵活的配置文件</li>
<li>自动从上一次处理的块 (chunk) 恢复并继续训练</li>
<li>自动分块 (chunking) 和进行输入的上下文扩展</li>
<li>多 GPU 训练</li>
<li>可以本地或者在 HPC 机器上运行</li>
<li>TIMIT 和 Librispeech 数据集的教程</li>
</ul>
<hr>
<h2 id="依赖包"><a href="#依赖包" class="headerlink" title="依赖包"></a>依赖包</h2><h3 id="Kaldi"><a href="#Kaldi" class="headerlink" title="Kaldi"></a>Kaldi</h3><p>我们首先需要安装Kaldi，读者请参考<a target="_blank" rel="noopener" href="https://kaldi-asr.org/doc/">官方文档</a>进行安装和学习Kaldi的基本用法。</p>
<p>安装好了之后需要把Kaldi的相关工具加到环境变量中，比如把下面的内容加到~/.bashrc下并且重新打开终端。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> KALDI_ROOT=/home/lili/codes/kaldi</span><br><span class="line">PATH=<span class="variable">$KALDI_ROOT</span>/tools/openfst:<span class="variable">$PATH</span></span><br><span class="line">PATH=<span class="variable">$KALDI_ROOT</span>/src/featbin:<span class="variable">$PATH</span></span><br><span class="line">PATH=<span class="variable">$KALDI_ROOT</span>/src/gmmbin:<span class="variable">$PATH</span></span><br><span class="line">PATH=<span class="variable">$KALDI_ROOT</span>/src/bin:<span class="variable">$PATH</span></span><br><span class="line">PATH=<span class="variable">$KALDI_ROOT</span>/src/nnetbin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> PATH</span><br></pre></td></tr></table></figure><br>读者需要把 KALDI_ROOT 设置成 kaldi 的<strong>根目录</strong>。如果运行 <code>copy-feats</code> 能出现帮助文档，则说明安装成功。</p>
<h3 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h3><p>目前 PyTorch-Kaldi 在 PyTorch1.0 和 0.4 做过测试，因此建议安装这两个版本的，为了提高效率，如果有 GPU 的话一定要安装 GPU 版本的 PyTorch。</p>
<blockquote>
<p>要保证 PyTorch 和 cuda 版本相兼容。根据经验，PyTorch 版本过新会导致 cuda 不兼容而报错。参考 <a target="_blank" rel="noopener" href="https://pytorch.org/get-started/previous-versions/">PyTorch 版本管理</a>。@Baileyswu</p>
</blockquote>
<h3 id="PyTorch-Kaldi"><a href="#PyTorch-Kaldi" class="headerlink" title="PyTorch-Kaldi"></a>PyTorch-Kaldi</h3><p>使用下面的代码进行安装，建议使用 virtualenv 来构建一个干净隔离的环境。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;mravanelli&#x2F;pytorch-kaldi</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure></h2><h2 id="TIMIT教程"><a href="#TIMIT教程" class="headerlink" title="TIMIT教程"></a>TIMIT教程</h2><h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><p>数据可以在<a target="_blank" rel="noopener" href="https://www.kaggle.com/mfekadu/darpa-timit-acousticphonetic-continuous-speech">这里</a>获取。</p>
<h3 id="使用Kaldi进行训练"><a href="#使用Kaldi进行训练" class="headerlink" title="使用Kaldi进行训练"></a>使用Kaldi进行训练</h3><h4 id="原理回顾"><a href="#原理回顾" class="headerlink" title="原理回顾"></a>原理回顾</h4><p>Kaldi 是传统的 HMM-GMM，我们希望用神经网络来替代其中的 GMM 声学模型部分。声学模型可以认为是计算概率 $P(X \vert q)$，这里 q 表示HMM 的状态，而 X 是观测(比如 MFCC 特征)，但是神经网络是区分性 (discriminative) 模型，它只能计算 $P(q \vert X)$，也就是给定观察，我们可以计算它属于某个状态的概率，也就是进行分类。当然，根据贝叶斯公式：</p>
<script type="math/tex; mode=display">
P(X | q)=\frac{P(q|X)P(X)}{P(q)} \propto \frac{P(q|X)}{P(q)}</script><p>因为 P(X) 是固定的，大家都一样，所以可以忽略。但是我们还是需要除以每个状态的先验概率 $P(q)$，这个先验概率可以从训练数据中统计出来。</p>
<p>那现在的问题是怎么获得训练数据，因为语音识别的训练数据是一个句子 (utterance) 的录音和对应的文字。状态是我们引入 HMM 模型的一个假设，世界上并没有一个实在的物体叫 HMM 状态。因此我们需要先训练 HMM-GMM 模型，通过强制对齐 (Force-Alignment) 算法让模型标注出最可能的状态序列。对齐后就有了状态和观察的对应关系，从而可以训练 HMM-DNN 模型了，Kaldi 中的 HMM-GMM 模型也是这样的原理。我们这里可以用 PyTorch-Kaldi 替代 Kaldi 自带的 DNN 模型，从而可以引入更加复杂的神经网络模型，而且实验起来速度更快，比较 PyTorch 是专门的神经网络框架，要实现一个新的网络结构非常简单。相比之下要在 Kaldi 里用 C++ 代码实现新的神经网络就复杂和低效(这里指的是开发效率，但是运行效率也可能是 PyTorch 更快，但是这个只是我的猜测)。当然我们也可以先训练 HMM-DNN，然后用 HMM-DNN 来进行强制对齐，因为 HMM-DNN 要比 HMM-GMM 的效果好，因此它的对齐也是更加准确。</p>
<h4 id="Kaldi训练"><a href="#Kaldi训练" class="headerlink" title="Kaldi训练"></a>Kaldi训练</h4><p>原理清楚了，下面我们来进行 Kaldi 的训练，但是训练前我们需要修改几个脚本。</p>
<p>读者如果有TIMIT数据集，在运行前需要修改一些脚本里的路径，下面是作者的修改，供参考。<br>首先需要修改cmd.sh，因为我是使用单机训练，所以需要把queue.pl改成run.pl。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">lili@lili-Precision-7720:~/codes/kaldi/egs/timit/s5$ git diff cmd.sh</span><br><span class="line">diff --git a/egs/timit/s5/cmd.sh b/egs/timit/s5/cmd.sh</span><br><span class="line">index 6c6dc88..7e3d909 100644</span><br><span class="line">--- a/egs/timit/s5/cmd.sh</span><br><span class="line">+++ b/egs/timit/s5/cmd.sh</span><br><span class="line">@@ -10,10 +10,10 @@</span><br><span class="line"> <span class="comment"># conf/queue.conf in http://kaldi-asr.org/doc/queue.html for more information,</span></span><br><span class="line"> <span class="comment"># or search for the string &#x27;default_config&#x27; in utils/queue.pl or utils/slurm.pl.</span></span><br><span class="line"> </span><br><span class="line">-<span class="built_in">export</span> train_cmd=<span class="string">&quot;queue.pl --mem 4G&quot;</span></span><br><span class="line">-<span class="built_in">export</span> decode_cmd=<span class="string">&quot;queue.pl --mem 4G&quot;</span></span><br><span class="line">+<span class="built_in">export</span> train_cmd=<span class="string">&quot;run.pl --mem 4G&quot;</span></span><br><span class="line">+<span class="built_in">export</span> decode_cmd=<span class="string">&quot;run.pl --mem 4G&quot;</span></span><br><span class="line"> <span class="comment"># the use of cuda_cmd is deprecated, used only in &#x27;nnet1&#x27;,</span></span><br><span class="line">-<span class="built_in">export</span> cuda_cmd=<span class="string">&quot;queue.pl --gpu 1&quot;</span></span><br><span class="line">+<span class="built_in">export</span> cuda_cmd=<span class="string">&quot;run.pl --gpu 1&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><br>接着修改修改 <code>run.sh</code> 里的数据路径 timit 变量修改成你自己的路径，另外我的机器CPU也不够多，因此把 <code>train_nj</code> 改小一点。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">lili@lili-Precision-7720:~/codes/kaldi/egs/timit/s5$ git diff run.sh</span><br><span class="line">diff --git a/egs/timit/s5/run.sh b/egs/timit/s5/run.sh</span><br><span class="line">index 58bd871..5c322cc 100755</span><br><span class="line">--- a/egs/timit/s5/run.sh</span><br><span class="line">+++ b/egs/timit/s5/run.sh</span><br><span class="line">@@ -28,7 +28,7 @@ numLeavesSGMM=7000</span><br><span class="line"> numGaussSGMM=9000</span><br><span class="line"> </span><br><span class="line"> feats_nj=10</span><br><span class="line">-train_nj=30</span><br><span class="line">+train_nj=8</span><br><span class="line"> decode_nj=5</span><br><span class="line"> </span><br><span class="line"> <span class="built_in">echo</span> ============================================================================</span><br><span class="line">@@ -36,8 +36,8 @@ <span class="built_in">echo</span> <span class="string">&quot;                Data &amp; Lexicon &amp; Language Preparation</span></span><br><span class="line"><span class="string"> echo ============================================================================</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string"> #timit=/export/corpora5/LDC/LDC93S1/timit/TIMIT # @JHU</span></span><br><span class="line"><span class="string">-timit=/mnt/matylda2/data/TIMIT/timit # @BUT</span></span><br><span class="line"><span class="string">-</span></span><br><span class="line"><span class="string">+#timit=/mnt/matylda2/data/TIMIT/timit # @BUT</span></span><br><span class="line"><span class="string">+timit=/home/lili/databak/ldc/LDC/timit/TIMIT</span></span><br><span class="line"><span class="string"> local/timit_data_prep.sh <span class="variable">$timit</span> || exit 1</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string"> local/timit_prepare_dict.sh</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>
<p>最后我们开始训练：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> kaldi/egs/timit/s5</span><br><span class="line">./run.sh</span><br><span class="line">./<span class="built_in">local</span>/nnet/run_dnn.sh</span><br></pre></td></tr></table></figure></p>
<h4 id="强制对齐"><a href="#强制对齐" class="headerlink" title="强制对齐"></a>强制对齐</h4><p>我们有两种选择，第一种使用 HMM-GMM 的对齐来训练 PyTorch-Kaldi，对于这种方式，训练数据已经对齐过了(因为训练 HMM-DNN 就需要对齐)，所以只需要对开发集和测试集再进行对齐：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> kaldi/egs/timit/s5</span><br><span class="line">steps/align_fmllr.sh --nj 4 data/dev data/lang exp/tri3 exp/tri3_ali_dev</span><br><span class="line">steps/align_fmllr.sh --nj 4 data/<span class="built_in">test</span> data/lang exp/tri3 exp/tri3_ali_test</span><br></pre></td></tr></table></figure><br>但是更好的是使用 HMM-DNN 来做对齐，作者使用的是这种方式，这就需要对训练集再做一次对齐了，因为之前的对齐是 HMM-GMM 做的，不是我们需要的。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">steps/nnet/align.sh --nj 4 data-fmllr-tri3/train data/lang exp/dnn4_pretrain-dbn_dnn exp/dnn4_pretrain-dbn_dnn_ali</span><br><span class="line">steps/nnet/align.sh --nj 4 data-fmllr-tri3/dev data/lang exp/dnn4_pretrain-dbn_dnn exp/dnn4_pretrain-dbn_dnn_ali_dev</span><br><span class="line">steps/nnet/align.sh --nj 4 data-fmllr-tri3/<span class="built_in">test</span> data/lang exp/dnn4_pretrain-dbn_dnn exp/dnn4_pretrain-dbn_dnn_ali_test</span><br></pre></td></tr></table></figure></p>
<h4 id="修改PyTorch-Kaldi的配置"><a href="#修改PyTorch-Kaldi的配置" class="headerlink" title="修改PyTorch-Kaldi的配置"></a>修改PyTorch-Kaldi的配置</h4><p>我们这里只介绍最简单的全连接网络(基本等价与 Kaldi 里的 DNN)，这个配置文件在 PyTorch-Kaldi 根目录下，位置是 <code>cfg/TIMIT_baselines/TIMIT_MLP_mfcc_basic.cfg</code>。从这个文件名我们可以猜测出这是使用 MFCC 特征的 MLP 模型，此外 cfg/TIMIT_baselines 目录下还有很多其它的模型。这个我们需要修改其中对齐后的目录等数据，请读者参考作者的修改进行修改。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">diff --git a/cfg/TIMIT_baselines/TIMIT_MLP_mfcc_basic.cfg b/cfg/TIMIT_baselines/TIMIT_MLP_mfcc_basic.cfg</span><br><span class="line">index 6f02075..6e5dc5d 100644</span><br><span class="line">--- a/cfg/TIMIT_baselines/TIMIT_MLP_mfcc_basic.cfg</span><br><span class="line">+++ b/cfg/TIMIT_baselines/TIMIT_MLP_mfcc_basic.cfg</span><br><span class="line">@@ -15,18 +15,18 @@ n_epochs_tr = 24</span><br><span class="line"> [dataset1]</span><br><span class="line"> data_name = TIMIT_tr</span><br><span class="line"> fea = fea_name=mfcc</span><br><span class="line">-	fea_lst=/home/mirco/kaldi-trunk/egs/timit/s5/data/train/feats.scp</span><br><span class="line">-	fea_opts=apply-cmvn --utt2spk=ark:/home/mirco/kaldi-trunk/egs/timit/s5/data/train/utt2spk  ark:/home/mirco/kaldi-trunk/egs/timit/s5/mfcc/cmvn_train.ark ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |</span><br><span class="line">+	fea_lst=/home/lili/codes/kaldi/egs/timit/s5/data/train/feats.scp</span><br><span class="line">+	fea_opts=apply-cmvn --utt2spk=ark:/home/lili/codes/kaldi/egs/timit/s5/data/train/utt2spk  ark:/home/lili/codes/kaldi/egs/timit/s5/mfcc/cmvn_train.ark ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |</span><br><span class="line"> 	cw_left=5</span><br><span class="line"> 	cw_right=5</span><br><span class="line"> 	</span><br><span class="line"> </span><br><span class="line"> lab = lab_name=lab_cd</span><br><span class="line">-	lab_folder=/home/mirco/kaldi-trunk/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali</span><br><span class="line">+	lab_folder=/home/lili/codes/kaldi/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali</span><br><span class="line"> 	lab_opts=ali-to-pdf</span><br><span class="line"> 	lab_count_file=auto</span><br><span class="line">-	lab_data_folder=/home/mirco/kaldi-trunk/egs/timit/s5/data/train/</span><br><span class="line">-	lab_graph=/home/mirco/kaldi-trunk/egs/timit/s5/exp/tri3/graph</span><br><span class="line">+	lab_data_folder=/home/lili/codes/kaldi/egs/timit/s5/data/train/</span><br><span class="line">+	lab_graph=/home/lili/codes/kaldi/egs/timit/s5/exp/tri3/graph</span><br><span class="line"> 	</span><br><span class="line"> </span><br><span class="line"> n_chunks = 5</span><br><span class="line">@@ -34,18 +34,18 @@ n_chunks = 5</span><br><span class="line"> [dataset2]</span><br><span class="line"> data_name = TIMIT_dev</span><br><span class="line"> fea = fea_name=mfcc</span><br><span class="line">-	fea_lst=/home/mirco/kaldi-trunk/egs/timit/s5/data/dev/feats.scp</span><br><span class="line">-	fea_opts=apply-cmvn --utt2spk=ark:/home/mirco/kaldi-trunk/egs/timit/s5/data/dev/utt2spk  ark:/home/mirco/kaldi-trunk/egs/timit/s5/mfcc/cmvn_dev.ark ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |</span><br><span class="line">+	fea_lst=/home/lili/codes/kaldi/egs/timit/s5/data/dev/feats.scp</span><br><span class="line">+	fea_opts=apply-cmvn --utt2spk=ark:/home/lili/codes/kaldi/egs/timit/s5/data/dev/utt2spk  ark:/home/lili/codes/kaldi/egs/timit/s5/mfcc/cmvn_dev.ark ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |</span><br><span class="line"> 	cw_left=5</span><br><span class="line"> 	cw_right=5</span><br><span class="line"> 	</span><br><span class="line"> </span><br><span class="line"> lab = lab_name=lab_cd</span><br><span class="line">-	lab_folder=/home/mirco/kaldi-trunk/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali_dev</span><br><span class="line">+	lab_folder=/home/lili/codes/kaldi/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali_dev</span><br><span class="line"> 	lab_opts=ali-to-pdf</span><br><span class="line"> 	lab_count_file=auto</span><br><span class="line">-	lab_data_folder=/home/mirco/kaldi-trunk/egs/timit/s5/data/dev/</span><br><span class="line">-	lab_graph=/home/mirco/kaldi-trunk/egs/timit/s5/exp/tri3/graph</span><br><span class="line">+	lab_data_folder=/home/lili/codes/kaldi/egs/timit/s5/data/dev/</span><br><span class="line">+	lab_graph=/home/lili/codes/kaldi/egs/timit/s5/exp/tri3/graph</span><br><span class="line"> 	</span><br><span class="line"> </span><br><span class="line"> n_chunks = 1</span><br><span class="line">@@ -53,18 +53,18 @@ n_chunks = 1</span><br><span class="line"> [dataset3]</span><br><span class="line"> data_name = TIMIT_test</span><br><span class="line"> fea = fea_name=mfcc</span><br><span class="line">-	fea_lst=/home/mirco/kaldi-trunk/egs/timit/s5/data/<span class="built_in">test</span>/feats.scp</span><br><span class="line">-	fea_opts=apply-cmvn --utt2spk=ark:/home/mirco/kaldi-trunk/egs/timit/s5/data/<span class="built_in">test</span>/utt2spk  ark:/home/mirco/kaldi-trunk/egs/timit/s5/mfcc/cmvn_test.ark ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |</span><br><span class="line">+	fea_lst=/home/lili/codes/kaldi/egs/timit/s5/data/<span class="built_in">test</span>/feats.scp</span><br><span class="line">+	fea_opts=apply-cmvn --utt2spk=ark:/home/lili/codes/kaldi/egs/timit/s5/data/<span class="built_in">test</span>/utt2spk  ark:/home/lili/codes/kaldi/egs/timit/s5/mfcc/cmvn_test.ark ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |</span><br><span class="line"> 	cw_left=5</span><br><span class="line"> 	cw_right=5</span><br><span class="line"> 	</span><br><span class="line"> </span><br><span class="line"> lab = lab_name=lab_cd</span><br><span class="line">-	lab_folder=/home/mirco/kaldi-trunk/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali_test</span><br><span class="line">+	lab_folder=/home/lili/codes/kaldi/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali_test</span><br><span class="line"> 	lab_opts=ali-to-pdf</span><br><span class="line"> 	lab_count_file=auto</span><br><span class="line">-	lab_data_folder=/home/mirco/kaldi-trunk/egs/timit/s5/data/<span class="built_in">test</span>/</span><br><span class="line">-	lab_graph=/home/mirco/kaldi-trunk/egs/timit/s5/exp/tri3/graph</span><br><span class="line">+	lab_data_folder=/home/lili/codes/kaldi/egs/timit/s5/data/<span class="built_in">test</span>/</span><br><span class="line">+	lab_graph=/home/lili/codes/kaldi/egs/timit/s5/exp/tri3/graph</span><br><span class="line"> 	</span><br><span class="line"> </span><br><span class="line"> n_chunks = 1</span><br></pre></td></tr></table></figure>
<p>看起来有点长，其实读者只需要搜索/home/mirco/kaldi-trunk，然后都替换成你自己的kaldi的root路径就行。注意：这里一定要用绝对路径而不能是~/这种。</p>
<p>这个配置文件后面我们再解释其含义。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run_exp.py cfg/TIMIT_baselines/TIMIT_MLP_mfcc_basic.cfg</span><br></pre></td></tr></table></figure>
<p>训练完成后会在目录 <code>exp/TIMIT_MLP_basic/</code> 下产生如下文件/目录：</p>
<ul>
<li>res.res</li>
</ul>
<p>每个 Epoch 在训练集和验证集上的 loss 和 error 以及最后测试的词错误率 (WER)。作者训练后得到的词错误率是 18%，每次训练因为随机初始化不同会有一点偏差。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">lili@lili-Precision-7720:~/codes/pytorch-kaldi$ tail exp/TIMIT_MLP_basic/res.res </span><br><span class="line">ep=16 tr=[<span class="string">&#x27;TIMIT_tr&#x27;</span>] loss=1.034 err=0.324 valid=TIMIT_dev loss=1.708 err=0.459 lr_architecture1=0.04 time(s)=43</span><br><span class="line">ep=17 tr=[<span class="string">&#x27;TIMIT_tr&#x27;</span>] loss=0.998 err=0.315 valid=TIMIT_dev loss=1.716 err=0.458 lr_architecture1=0.04 time(s)=42</span><br><span class="line">ep=18 tr=[<span class="string">&#x27;TIMIT_tr&#x27;</span>] loss=0.980 err=0.309 valid=TIMIT_dev loss=1.727 err=0.458 lr_architecture1=0.04 time(s)=42</span><br><span class="line">ep=19 tr=[<span class="string">&#x27;TIMIT_tr&#x27;</span>] loss=0.964 err=0.306 valid=TIMIT_dev loss=1.733 err=0.457 lr_architecture1=0.04 time(s)=43</span><br><span class="line">ep=20 tr=[<span class="string">&#x27;TIMIT_tr&#x27;</span>] loss=0.950 err=0.302 valid=TIMIT_dev loss=1.744 err=0.458 lr_architecture1=0.04 time(s)=45</span><br><span class="line">ep=21 tr=[<span class="string">&#x27;TIMIT_tr&#x27;</span>] loss=0.908 err=0.290 valid=TIMIT_dev loss=1.722 err=0.452 lr_architecture1=0.02 time(s)=45</span><br><span class="line">ep=22 tr=[<span class="string">&#x27;TIMIT_tr&#x27;</span>] loss=0.888 err=0.284 valid=TIMIT_dev loss=1.735 err=0.453 lr_architecture1=0.02 time(s)=44</span><br><span class="line">ep=23 tr=[<span class="string">&#x27;TIMIT_tr&#x27;</span>] loss=0.864 err=0.277 valid=TIMIT_dev loss=1.719 err=0.450 lr_architecture1=0.01 time(s)=44</span><br><span class="line">%WER 18.0 | 192 7215 | 84.9 11.4 3.6 2.9 18.0 99.5 | -1.324 | /home/lili/codes/pytorch-kaldi/exp/TIMIT_MLP_basic/decode_TIMIT_test_out_dnn1/score_4/ctm_39phn.filt.sys</span><br></pre></td></tr></table></figure></p>
<ul>
<li>log.log</li>
</ul>
<p>日志，包括错误和警告信息。如果出现问题，可以首先看看这个文件。</p>
<ul>
<li>conf.cfg</li>
</ul>
<p>配置的一个拷贝</p>
<ul>
<li>model.svg</li>
</ul>
<p>网络的结构图，如下图所示：</p>
<p> <a name='model'><img src="/2020/06/pytorch-kaldi/model.svg" class=""></a><br><em>图：网络的结构图</em></p>
<ul>
<li>exp_files目录</li>
</ul>
<p>这个目录包含很多文件，用于描述每一个 Epoch 的训练详细信息。比如后缀为 <code>.info</code> 的文件说明块 (chunk) 的信息，后面我们会介绍什么叫块。<code>.cfg</code> 是每个快的配置信息。<code>.lst</code> 列举这个块使用的特征文件。</p>
<ul>
<li><p>generated_outputs目录<br>包括训练和验证的准确率和 loss 随 epoch 的变化，比如 loss 如下图所示：</p>
<p><a name='loss'><img src="/2020/06/pytorch-kaldi/loss.png" class=""></a><br><em>图：训练过程中loss的变化图</em></p>
</li>
</ul>
<h3 id="使用其它特征"><a href="#使用其它特征" class="headerlink" title="使用其它特征"></a>使用其它特征</h3><p>如果需要使用其它特征，比如 Filter Bank 特征，我们需要做如下的修改然后重新进行 Kalid 的训练。我们需要找到 <code>KALDI_ROOT/egs/timit/s5/run.sh</code> 然后把</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mfccdir=mfcc</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> train dev <span class="built_in">test</span>; <span class="keyword">do</span></span><br><span class="line">  steps/make_mfcc.sh --cmd <span class="string">&quot;<span class="variable">$train_cmd</span>&quot;</span> --nj <span class="variable">$feats_nj</span> data/<span class="variable">$x</span> exp/make_mfcc/<span class="variable">$x</span> <span class="variable">$mfccdir</span></span><br><span class="line">  steps/compute_cmvn_stats.sh data/<span class="variable">$x</span> exp/make_mfcc/<span class="variable">$x</span> <span class="variable">$mfccdir</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>改成：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">feadir=fbank</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> train dev <span class="built_in">test</span>; <span class="keyword">do</span></span><br><span class="line">  steps/make_fbank.sh --cmd <span class="string">&quot;<span class="variable">$train_cmd</span>&quot;</span> --nj <span class="variable">$feats_nj</span> data/<span class="variable">$x</span> exp/make_fbank/<span class="variable">$x</span> <span class="variable">$feadir</span></span><br><span class="line">  steps/compute_cmvn_stats.sh data/<span class="variable">$x</span> exp/make_fbank/<span class="variable">$x</span> <span class="variable">$feadir</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><br>接着修改 Pytorch-Kaldi 的配置(比如 <code>cfg/TIMIT_baselines/TIMIT_MLP_mfcc_basic.cfg</code>)，把 <code>fea_lst</code> 改成fbank特征的路径。</p>
<p>如果需要使用 fmllr 特征(使用了说话人自适应技术)，那么前面完整的 kaldi 脚本已经提取过了这个特征，因此不需要再次提取。如果没有运行完整的脚本，需要完整的运行它一次。</p>
<h3 id="使用其它模型"><a href="#使用其它模型" class="headerlink" title="使用其它模型"></a>使用其它模型</h3><p>在 <code>cfg/TIMIT_baselines/</code> 目录下还有很多模型，比如 CNN、LSTM和GRU 等，这里就不介绍了。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>在 TIMIT 数据集上使用不同方法的实验结果如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>mfcc</th>
<th>fbank</th>
<th>fMLLR</th>
</tr>
</thead>
<tbody>
<tr>
<td>Kaldi DNN Baseline</td>
<td>——-</td>
<td>———</td>
<td>18.5</td>
</tr>
<tr>
<td>MLP</td>
<td>18.2</td>
<td>18.7</td>
<td>16.7</td>
</tr>
<tr>
<td>RNN</td>
<td>17.7</td>
<td>17.2</td>
<td>15.9</td>
</tr>
<tr>
<td>SRU</td>
<td>——-</td>
<td>16.6</td>
<td>——-</td>
</tr>
<tr>
<td>LSTM</td>
<td>15.1</td>
<td>14.3</td>
<td>14.5</td>
</tr>
<tr>
<td>GRU</td>
<td>16.0</td>
<td>15.2</td>
<td>14.9</td>
</tr>
<tr>
<td>li-GRU</td>
<td><strong>15.5</strong></td>
<td><strong>14.9</strong></td>
<td><strong>14.2</strong></td>
</tr>
</tbody>
</table>
</div>
<p>从上表可以看出，fMLLR 比 mfcc 和 fbank 的特征效果要好，因为它使用了说话人自适应 (Speaker Adaptation) 的技术。从模型的角度来看 LSTM、GRU 比 MLP 要好，而 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.10225.pdf">Li-GRU</a> 模型比它们还要更好一点。</p>
<p>如果把三个特征都融合起来，使用Li-GRU可以得到更好的结果，词错误率是13.8%。感兴趣的读者可以参考 <code>cfg/TIMI_baselines/TIMIT_mfcc_fbank_fmllr_liGRU_best.cfg</code>。</p>
<h2 id="Librispeech教程"><a href="#Librispeech教程" class="headerlink" title="Librispeech教程"></a>Librispeech教程</h2><p>官网还提供了Librispeech教程，这个数据集是免费的，读者可以在<a target="_blank" rel="noopener" href="http://www.openslr.org/12/">这里</a>下载。由于磁盘空间限制，之前我下载和训练过的Librispeech数据都删除了，所以我没有用PyTorch-Kaldi跑过，因此也就不介绍了。<br>但是原理都差不多，感兴趣的读者请参考<a target="_blank" rel="noopener" href="https://github.com/mravanelli/pytorch-kaldi#librispeech-tutorial">官网教程</a>。</p>
<hr>
<h2 id="PyTorch-Kaldi的工作过程"><a href="#PyTorch-Kaldi的工作过程" class="headerlink" title="PyTorch-Kaldi的工作过程"></a>PyTorch-Kaldi的工作过程</h2><p>最重要的是 <code>run_exp.py</code> 文件，它用来执行训练、验证、forward 和解码。训练会分成很多个 Epoch，一个 Epoch 训练完成后会在验证集上进行验证。训练结束后会执行 forward，也就是在测试数据集上根据输入特征计算后验概率 $p(q \vert X)$，这里 X 是特征(比如 mfcc)。但是为了在 HMM 里使用，我们需要似然概率 $p(X \vert q)$，因此我们还需要除以先验概率 $p(q)$。最后使用 Kaldi 来解码，输出最终的文本。注意：特征提取是Kaldi完成，前面已经做过了(包括测试集)，而计算似然 $p(X \vert q)$ 是 PyTorch-Kaldi 来完成的，最后的解码又是由 Kaldi 来做的。</p>
<p><code>run_exp.py</code> 的输入是一个配置文件(比如我们前面用到的<code>TIMIT_MLP_mfcc_basic.cfg</code>)，这个配置文件包含了训练神经网络的所有参数。因为训练数据可能很大，PyTorch-Kaldi 会把整个数据集划分成更小的块 (chunk)，以便能够放到内存里训练。<code>run_exp.py</code> 会调用<code>run_nn</code> 函数(在 <code>core.py</code> 里)来训练一个块的数据，<code>run_nn</code> 函数也需要一个类似的配置文件(比如 <code>exp/TIMIT_MLP_basic/exp_files/train_TIMIT_tr_ep00_ck1.cfg</code> )。这个文件里会指明训练哪些数据(比如 <code>fea_lst=exp/TIMIT_MLP_basic/exp_files/train_TIMIT_tr_ep00_ck1_mfcc.lst</code>)，同时训练结果比如 loss 等信息也会输出到 info 文件里(比如 <code>exp/TIMIT_MLP_basic/exp_files/train_TIMIT_tr_ep00_ck1.info</code>)。</p>
<p>比如作者训练时 <code>exp/TIMIT_MLP_basic/exp_files/train_TIMIT_tr_ep00_ck1_mfcc.lst</code> 的内容如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ head exp/TIMIT_MLP_basic/exp_files/train_TIMIT_tr_ep00_ck1_mfcc.lst</span><br><span class="line">MAEB0_SX450 /home/lili/codes/kaldi/egs/timit/s5/mfcc/raw_mfcc_train.4.ark:32153</span><br><span class="line">MRWA0_SX163 /home/lili/codes/kaldi/egs/timit/s5/mfcc/raw_mfcc_train.9.ark:862231</span><br><span class="line">MMGC0_SI1935 /home/lili/codes/kaldi/egs/timit/s5/mfcc/raw_mfcc_train.8.ark:15925</span><br><span class="line">MRLJ1_SI2301 /home/lili/codes/kaldi/egs/timit/s5/mfcc/raw_mfcc_train.9.ark:355566</span><br><span class="line">MRJB1_SX390 /home/lili/codes/kaldi/egs/timit/s5/mfcc/raw_mfcc_train.9.ark:109739</span><br><span class="line">FLAC0_SX361 /home/lili/codes/kaldi/egs/timit/s5/mfcc/raw_mfcc_train.2.ark:786772</span><br><span class="line">FMBG0_SI1790 /home/lili/codes/kaldi/egs/timit/s5/mfcc/raw_mfcc_train.2.ark:1266225</span><br><span class="line">FTBW0_SX85 /home/lili/codes/kaldi/egs/timit/s5/mfcc/raw_mfcc_train.3.ark:1273832</span><br><span class="line">MDDC0_SX339 /home/lili/codes/kaldi/egs/timit/s5/mfcc/raw_mfcc_train.4.ark:1427498</span><br><span class="line">FPAF0_SX244 /home/lili/codes/kaldi/egs/timit/s5/mfcc/raw_mfcc_train.3.ark:207223</span><br></pre></td></tr></table></figure>
<p><code>exp/TIMIT_MLP_basic/exp_files/train_TIMIT_tr_ep00_ck1.info</code> 的内容如下：</p>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cat exp/TIMIT_MLP_basic/exp_files/train_TIMIT_tr_ep00_ck1.info</span><br><span class="line">[results]</span><br><span class="line">loss=3.6573577</span><br><span class="line">err=0.7678323</span><br><span class="line">elapsed_time_chunk=8.613296</span><br><span class="line"></span><br></pre></td></tr></table></figure></h2><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>这里有两种配置文件：全局的配置文件(比如cfg/TIMIT_baselines/TIMIT_MLP_mfcc_basic.cfg)和块的配置文件(比如exp/TIMIT_MLP_basic/exp_files/train_TIMIT_tr_ep00_ck1.cfg)。它们都是ini文件，使用configparser库来parse。全局配置文件包含很多节(section，在ini文件里用[section-name]开始一个section)，它说明了训练、验证、forward和解码的过程。块配置文件和全局配置文件很类似，我们先介绍全局配置文件，这里以cfg/TIMIT_baselines/TIMIT_MLP_mfcc_basic.cfg为例。</p>
<h3 id="cfg-proto"><a href="#cfg-proto" class="headerlink" title="cfg_proto"></a>cfg_proto</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[cfg_proto]</span><br><span class="line">cfg_proto &#x3D; proto&#x2F;global.proto</span><br><span class="line">cfg_proto_chunk &#x3D; proto&#x2F;global_chunk.proto</span><br></pre></td></tr></table></figure>
<p>cfg_proto节指明全局配置文件和块配置文件的结构，我们看一下proto/global.proto</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[cfg_proto]</span><br><span class="line">cfg_proto&#x3D;path</span><br><span class="line">cfg_proto_chunk&#x3D;path</span><br><span class="line"></span><br><span class="line">[exp]</span><br><span class="line">cmd&#x3D;str</span><br><span class="line">run_nn_script&#x3D;str</span><br><span class="line">out_folder&#x3D;str</span><br><span class="line">seed&#x3D;int(-inf,inf)</span><br><span class="line">use_cuda&#x3D;bool</span><br><span class="line">multi_gpu&#x3D;bool</span><br><span class="line">save_gpumem&#x3D;bool</span><br><span class="line">N_epochs_tr&#x3D;int(1,inf)</span><br></pre></td></tr></table></figure>
<p>这个 <code>global.proto</code> 可以认为定义了 <code>TIMIT_MLP_mfcc_basic.cfg</code> 的结构 (schema)。比如它定义了 <code>cfg_proto</code> 节有两个配置项：<code>cfg_proto</code> 和 <code>cfg_proto_chunk</code>，它们的值是 path(路径)。因此我们在 <code>TIMIT_MLP_mfcc_basic.cfg</code> 的 <code>cfg_proto</code> 节只能配置 <code>cfg_proto</code> 和 <code>cfg_proto_chunk</code>。</p>
<p>类似的，<code>global.proto</code> 定义了 <code>exp</code> 节包含 <code>cmd</code>，它是一个字符串；<code>seed</code>，它是一个负无穷(-inf)到无穷(inf)的整数；<code>N_epochs_tr</code>，它是一个1到无穷的整数。</p>
<p>因此我们可以在 <code>TIMIT_MLP_mfcc_basic.cfg</code> 里做如下定义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[exp]</span><br><span class="line">cmd &#x3D; </span><br><span class="line">run_nn_script &#x3D; run_nn</span><br><span class="line">out_folder &#x3D; exp&#x2F;TIMIT_MLP_basic</span><br><span class="line">seed &#x3D; 1234</span><br><span class="line">use_cuda &#x3D; True</span><br><span class="line">multi_gpu &#x3D; False</span><br><span class="line">save_gpumem &#x3D; False</span><br><span class="line">n_epochs_tr &#x3D; 24</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>exp 节是实验的一些全局配置。这些配置的含义我们大致可以猜测出来：cmd是分布式训练时的脚本，我们这里设置为空即可；run_nn_script 是块的训练函数，这里是 run_nn(core.py)；out_folder 是实验的输出目录；seed 是随机种子；use_cuda 是否使用 CUDA；multi-gpu 表示是否多 GPU 训练；n_epochs_tr 表示训练的 epoch 数。</p>
<p>我们这里需要修改的一般就是 use_cuda，如果没有 GPU 则需要把它改成False。下面我们只介绍 <code>TIMIT_MLP_mfcc_basic.cfg</code> 的各个节，它的结构就不介绍了。</p>
<h3 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h3><p><code>dataset</code> 用于配置数据，我们这里配置训练、验证和测试3个数据集，分别用 dataset1、dataset2 和 dataset3 表示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">[dataset1]</span><br><span class="line">data_name &#x3D; TIMIT_tr</span><br><span class="line">fea &#x3D; fea_name&#x3D;mfcc</span><br><span class="line">	fea_lst&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;data&#x2F;train&#x2F;feats.scp</span><br><span class="line">	fea_opts&#x3D;apply-cmvn --utt2spk&#x3D;ark:&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;data&#x2F;train&#x2F;utt2spk  ark:&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;mfcc&#x2F;cmvn_train.ark ark:- ark:- | add-deltas --delta-order&#x3D;2 ark:- ark:- |</span><br><span class="line">	cw_left&#x3D;5</span><br><span class="line">	cw_right&#x3D;5</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">lab &#x3D; lab_name&#x3D;lab_cd</span><br><span class="line">	lab_folder&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;exp&#x2F;dnn4_pretrain-dbn_dnn_ali</span><br><span class="line">	lab_opts&#x3D;ali-to-pdf</span><br><span class="line">	lab_count_file&#x3D;auto</span><br><span class="line">	lab_data_folder&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;data&#x2F;train&#x2F;</span><br><span class="line">	lab_graph&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;exp&#x2F;tri3&#x2F;graph</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">n_chunks &#x3D; 5</span><br><span class="line"></span><br><span class="line">[dataset2]</span><br><span class="line">data_name &#x3D; TIMIT_dev</span><br><span class="line">fea &#x3D; fea_name&#x3D;mfcc</span><br><span class="line">	fea_lst&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;data&#x2F;dev&#x2F;feats.scp</span><br><span class="line">	fea_opts&#x3D;apply-cmvn --utt2spk&#x3D;ark:&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;data&#x2F;dev&#x2F;utt2spk  ark:&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;mfcc&#x2F;cmvn_dev.ark ark:- ark:- | add-deltas --delta-order&#x3D;2 ark:- ark:- |</span><br><span class="line">	cw_left&#x3D;5</span><br><span class="line">	cw_right&#x3D;5</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">lab &#x3D; lab_name&#x3D;lab_cd</span><br><span class="line">	lab_folder&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;exp&#x2F;dnn4_pretrain-dbn_dnn_ali_dev</span><br><span class="line">	lab_opts&#x3D;ali-to-pdf</span><br><span class="line">	lab_count_file&#x3D;auto</span><br><span class="line">	lab_data_folder&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;data&#x2F;dev&#x2F;</span><br><span class="line">	lab_graph&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;exp&#x2F;tri3&#x2F;graph</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">n_chunks &#x3D; 1</span><br><span class="line"></span><br><span class="line">[dataset3]</span><br><span class="line">data_name &#x3D; TIMIT_test</span><br><span class="line">fea &#x3D; fea_name&#x3D;mfcc</span><br><span class="line">	fea_lst&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;data&#x2F;test&#x2F;feats.scp</span><br><span class="line">	fea_opts&#x3D;apply-cmvn --utt2spk&#x3D;ark:&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;data&#x2F;test&#x2F;utt2spk  ark:&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;mfcc&#x2F;cmvn_test.ark ark:- ark:- | add-deltas --delta-order&#x3D;2 ark:- ark:- |</span><br><span class="line">	cw_left&#x3D;5</span><br><span class="line">	cw_right&#x3D;5</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">lab &#x3D; lab_name&#x3D;lab_cd</span><br><span class="line">	lab_folder&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;exp&#x2F;dnn4_pretrain-dbn_dnn_ali_test</span><br><span class="line">	lab_opts&#x3D;ali-to-pdf</span><br><span class="line">	lab_count_file&#x3D;auto</span><br><span class="line">	lab_data_folder&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;data&#x2F;test&#x2F;</span><br><span class="line">	lab_graph&#x3D;&#x2F;home&#x2F;lili&#x2F;codes&#x2F;kaldi&#x2F;egs&#x2F;timit&#x2F;s5&#x2F;exp&#x2F;tri3&#x2F;graph</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">n_chunks &#x3D; 1</span><br></pre></td></tr></table></figure>
<p>每个 <code>dataset</code> 有一个名字，比如 <code>TIMIT_tr</code>。接下来是 <code>fea</code>，它用来配置特征(神经网络的输入)，这个配置又有很多子配置项。<code>fea_name</code> 给它起个名字。而 <code>fea_lst</code> 表示特征 <code>scp</code> 文件。它指明每个 <code>utterance</code> 对应的特征在 <code>ark</code> 文件里的位置，不熟悉的读者请参考 <code>Kaldi</code> 文档或者本书前面的内容。<code>fea_opts</code> 表示对原始的特征文件执行的一些命令，比如 <code>apply-cmvn</code> 表示对原始的 MFCC 特征进行均值和方差的归一化。<code>cw_left</code> 和 <code>cw_right=5</code> 表示除了当前帧，我们还使用左右各 5 帧也就是共 11 帧的特征来预测。使用当前帧左右的数据这对于MLP来说是很有效的，但是对于 LSTM 或者 GRU 来说是不必要的，比如在 <code>cfg/TIMIT_baselines/TIMIT_LSTM_mfcc.cfg</code> 里 <code>cw_left=0</code>。</p>
<p>而 lab 用来配置标签(上下文相关因子是 PyTorch-Kaldi 的输出)，它也有很多子配置项。lab_name 是名字，lab_folder 指定对齐结果的目录。 “lab_opts=ali-to-pdf” 表示使用标准的上下文相关的因子表示(cd phone,contextual dependent phone)；如果我们不想考虑上下文(训练数据很少的时候)可以使用 “<code>lab_opts=ali-to-phones --per-frame=true</code>“。lab_count_file 是用于指定因子的先验概率的文件，auto 让 PyTorch-Kaldi 自己去计算。lab_data_folder 指明数据的位置，注意它是 kaldi 数据的位置，而不是 PyTorch-Kaldi 的数据。</p>
<p>因为训练数据通常很大，不能全部放到内存里，因此我们需要用 n_chunks 把所有数据切分成 n_chunks 个块。这里因为 TIMIT 不大，所以只需要分成 5 个块。而验证和测试的时候数据量不大，所以 n_chunks=1，也就是全部放到内存。如果我们看 Librispeech 的配置，因为它的数据比较大，所以它配置成 N_chunks=50。</p>
<p>通常我们让一个块包含 1 到 2 个小时的语音数据。</p>
<h3 id="data-use"><a href="#data-use" class="headerlink" title="data_use"></a>data_use</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[data_use]</span><br><span class="line">train_with &#x3D; TIMIT_tr</span><br><span class="line">valid_with &#x3D; TIMIT_dev</span><br><span class="line">forward_with &#x3D; TIMIT_test</span><br></pre></td></tr></table></figure>
<p>data_use 指定训练、验证和 forward (其实就是测试)使用的数据集的名字，TIMIT_tr、TIMIT_dev 和 TIMIT_test 就是我们之前在dataset里定义的。</p>
<h3 id="batches"><a href="#batches" class="headerlink" title="batches"></a>batches</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_size_train &#x3D; 128</span><br><span class="line">max_seq_length_train &#x3D; 1000</span><br><span class="line">increase_seq_length_train &#x3D; False</span><br><span class="line">start_seq_len_train &#x3D; 100</span><br><span class="line">multply_factor_seq_len_train &#x3D; 2</span><br><span class="line">batch_size_valid &#x3D; 128</span><br><span class="line">max_seq_length_valid &#x3D; 1000</span><br></pre></td></tr></table></figure>
<p><code>batch_size_train</code> 指定训练的 batch 大小。<code>max_seq_length_train</code> 配置最大的句子长度，如果太长，LSTM 等模型可能会内存不足从而出现OOM的问题。我们也可以逐步增加句子的长度，先让模型学习比较短的上下文，然后逐步增加长度。如果这样，我们可以设置 <code>increase_seq_length_train</code> 为 True，这个时候第一个 epoch 的最大长度会设置成 <code>start_seq_len_train</code> (100)，然后第二个 epoch 设置成 <code>start_seq_len_train * multply_factor_seq_len_train</code>(200)，……，直到 <code>max_seq_length_train</code>。这样的好处是先学习比较短的上下文，然后学习较长的上下文依赖。实验发现这种策略可以提高模型的学习效率。</p>
<p>类似的 <code>batch_size_valid</code> 和 <code>max_seq_length_valid</code> 指定验证集的batch大小和最大句子长度。</p>
<h3 id="architecture"><a href="#architecture" class="headerlink" title="architecture"></a>architecture</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[architecture1]</span><br><span class="line">arch_name &#x3D; MLP_layers1</span><br><span class="line">arch_proto &#x3D; proto&#x2F;MLP.proto</span><br><span class="line">arch_library &#x3D; neural_networks</span><br><span class="line">arch_class &#x3D; MLP</span><br><span class="line">arch_pretrain_file &#x3D; none</span><br><span class="line">arch_freeze &#x3D; False</span><br><span class="line">arch_seq_model &#x3D; False</span><br><span class="line">dnn_lay &#x3D; 1024,1024,1024,1024,N_out_lab_cd</span><br><span class="line">dnn_drop &#x3D; 0.15,0.15,0.15,0.15,0.0</span><br><span class="line">dnn_use_laynorm_inp &#x3D; False</span><br><span class="line">dnn_use_batchnorm_inp &#x3D; False</span><br><span class="line">dnn_use_batchnorm &#x3D; True,True,True,True,False</span><br><span class="line">dnn_use_laynorm &#x3D; False,False,False,False,False</span><br><span class="line">dnn_act &#x3D; relu,relu,relu,relu,softmax</span><br><span class="line">arch_lr &#x3D; 0.08</span><br><span class="line">arch_halving_factor &#x3D; 0.5</span><br><span class="line">arch_improvement_threshold &#x3D; 0.001</span><br><span class="line">arch_opt &#x3D; sgd</span><br><span class="line">opt_momentum &#x3D; 0.0</span><br><span class="line">opt_weight_decay &#x3D; 0.0</span><br><span class="line">opt_dampening &#x3D; 0.0</span><br><span class="line">opt_nesterov &#x3D; False</span><br></pre></td></tr></table></figure>
<p>architecture 定义神经网络模型(的超参数)。arch_name 就是起一个名字，后面会用到。</p>
<p>arch_proto 指定网络结构的定义 (schema) 为文件 <code>proto/MLP.proto</code>。因为不同的神经网络需要不同的配置，所以这里还需要通过arch_proto 引入不同网络的配置。而 global.proto 里只定义所有网络模型都会用到的配置，这些配置都是以 arch_ 开头。我们先看这些 arch_ 开头的配置，然后再看 MLP.proto 新引入的与特定网络相关的配置(MLP.proto 里的配置都是 dnn_ 开头)。</p>
<ul>
<li>arch_name 名字</li>
<li>arch_proto 具体的网络proto路径</li>
<li><p>arch_library 实现这个网络的Python类所在的文件</p>
<p>比如MLP类是在neural_networks.py里实现的。</p>
</li>
<li><p>arch_class 实现这个网络的类(PyTorch 的 nn.Module的子类)，这里是MLP。</p>
<p> 注意：<code>neural_networks.py</code> 除了实现 MLP 还实现其它网络结果比如 LSTM。arch_library 和 arch_class 就告诉了 PyTorch 使用那个模块的哪个类来定义神经网络。</p>
</li>
<li><p>arch_pretrain_file 用于指定之前预训练的模型的路径</p>
</li>
</ul>
<p>比如我先训练一个两层的MLP，然后再训练三层的时候可以使用之前的参数作为初始值。</p>
<ul>
<li>arch_freeze 训练模型时是否固定(freeze)参数</li>
</ul>
<p>这看起来似乎没什么用，毕竟我们训练模型不就是为了调整参数吗？我也不是特别明白，也许是多个模型融合时我们可以先固定一个然后训练另一个？或者是我们固定预训练的arch_pretrain_file中的参数，只训练后面新加的模型的参数？</p>
<ul>
<li><p>arch_seq_model 是否序列模型</p>
<p>这个参数告诉 PyTorch 你的模型是否序列模型，如果是多个模型的融合的话，只要有一个序列模型(比如 LSTM)，那么整个模型都是序列模型。如果不是序列模型的话，给神经网络的训练数据就不用给一个序列，这样它可以随机的打散一个句子的多个因子，从而每次训练这个句子都不太一样，这样效果会更好一点。但是如果是序列模型，那么给定的句子就必须是真正的序列。</p>
</li>
<li><p>arch_lr learning rate</p>
</li>
<li><p>arch_halving_factor 0.5</p>
<p>如果当前 epoch 比前一个 epoch 在验证集上的提高小于arch_improvement_threshold，则把 learning rate 乘以arch_halving_factor(0.5)，也就是减小 learning rate。</p>
</li>
<li><p>arch_improvement_threshold</p>
<p>参考上面的说明。</p>
</li>
<li><p>arch_opt sgd 优化算法</p>
</li>
</ul>
<p>接下来的 opt_ 开头的参数是 sgd 的一些子配置，它的定义在 proto/sgd.proto。不同的优化算法有不同的子配置项目，比如 proto/sgd.proto如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[proto]</span><br><span class="line">opt_momentum&#x3D;float(0,inf)</span><br><span class="line">opt_weight_decay&#x3D;float(0,inf)</span><br><span class="line">opt_dampening&#x3D;float(0,inf)</span><br><span class="line">opt_nesterov&#x3D;bool</span><br></pre></td></tr></table></figure><br>从名字我们可以猜测，opt_momentum 是冲量的大小，我们这里配置是 0，因此就是没有冲量的最普通的 sgd。opt_weight_decay 是 weight_decay 的权重。opt_nesterov 说明是否 nesterov 冲量。opt_dampening 我不知道是什么，我只搜索到<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/issues/6">这个ISSUE</a>，似乎是一个需要废弃的东西，<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html">sgd的文档</a>好像也能看到 dampening。关于优化算法，读者可以参考基础篇或者参考<a target="_blank" rel="noopener" href="http://cs231n.github.io/neural-networks-3/#update">cs231n的note</a></p>
<p>看完了通用的 architecture 配置，我们再来看 MLP.proto 里的具体的网络配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dnn_lay &#x3D; 1024,1024,1024,1024,N_out_lab_cd</span><br><span class="line">dnn_drop &#x3D; 0.15,0.15,0.15,0.15,0.0</span><br><span class="line">dnn_use_laynorm_inp &#x3D; False</span><br><span class="line">dnn_use_batchnorm_inp &#x3D; False</span><br><span class="line">dnn_use_batchnorm &#x3D; True,True,True,True,False</span><br><span class="line">dnn_use_laynorm &#x3D; False,False,False,False,False</span><br><span class="line">dnn_act &#x3D; relu,relu,relu,relu,softmax</span><br></pre></td></tr></table></figure>
<p>我们可以从名字中猜测出来它们的含义(如果猜不出来就只能看源代码了，位置在 neural_networks.py 的MLP类)。dnn_lay 定义了5个全连接层，前4层的隐单元个数是1024，而最后一层的个数是一个特殊的 N_out_lab_cd，它表示上下文相关的因子的数量，也就是分类器的分类个数。dnn_drop 表示这 5 层的 dropout。dnn_use_laynorm_inp 表示是否对输入进行 layernorm，dnn_use_batchnorm_inp 表示是否对输入进行batchnorm。dnn_use_batchnorm 表示对 5 个全连接层是否使用 batchnorm。dnn_use_laynorm 表示对5个全连接层是否使用 layernorm。dnn_act表示每一层的激活函数，除了最后一层是 softmax，前面 4 层都是 relu。</p>
<h3 id="model"><a href="#model" class="headerlink" title="model"></a>model</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[model]</span><br><span class="line">model_proto &#x3D; proto&#x2F;model.proto</span><br><span class="line">model &#x3D; out_dnn1&#x3D;compute(MLP_layers1,mfcc)</span><br><span class="line">	loss_final&#x3D;cost_nll(out_dnn1,lab_cd)</span><br><span class="line">	err_final&#x3D;cost_err(out_dnn1,lab_cd)</span><br></pre></td></tr></table></figure>
<p>model 定义输出和损失函数，out_dnn1=compute(MLP_layers,mfcc) 的意思是把 mfcc 特征(前面的section定义过)输入 MLP_layers1(前面定义的architecture)，从而计算出分类的概率(softmax)，把它记为 out_dnn1，然后用 out_dnn1 和 lab_cd 计算交叉熵损失函数 (cost_nll)，同时也计算错误率 (cost_err)。当然这个配置文件的 model 比较简单，我们看一个比较复杂的例子 (cfg/TIMIT_baselines/TIMIT_mfcc_fbank_fmllr_liGRU_best.cfg)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[model]</span><br><span class="line">model_proto&#x3D;proto&#x2F;model.proto</span><br><span class="line">model:conc1&#x3D;concatenate(mfcc,fbank)</span><br><span class="line">      conc2&#x3D;concatenate(conc1,fmllr)</span><br><span class="line">      out_dnn1&#x3D;compute(MLP_layers_first,conc2)</span><br><span class="line">      out_dnn2&#x3D;compute(liGRU_layers,out_dnn1)</span><br><span class="line">      out_dnn3&#x3D;compute(MLP_layers_second,out_dnn2)</span><br><span class="line">      out_dnn4&#x3D;compute(MLP_layers_last,out_dnn3)</span><br><span class="line">      out_dnn5&#x3D;compute(MLP_layers_last2,out_dnn3)</span><br><span class="line">      loss_mono&#x3D;cost_nll(out_dnn5,lab_mono)</span><br><span class="line">      loss_mono_w&#x3D;mult_constant(loss_mono,1.0)</span><br><span class="line">      loss_cd&#x3D;cost_nll(out_dnn4,lab_cd)</span><br><span class="line">      loss_final&#x3D;sum(loss_cd,loss_mono_w)</span><br><span class="line">      err_final&#x3D;cost_err(out_dnn4,lab_cd)</span><br></pre></td></tr></table></figure><br>在上面的例子里，我们把 mfcc、fbank 和 fmllr 特征拼接成一个大的特征，然后使用一个 MLP_layers_first(这是一个全连接层)，然后再使用 liGRU(liGRU_layers)，然后再加一个全连接层得到 out_dnn3。out_dnn3 再用 MLP_layers_last 得到上下文相关因子的分类(MLP_layers_last 的输出是 N_out_lab_cd)；out_dnn 用 out_dnn4 得到上下文无关的因子分类(MLP_layers_last2 的输出是 N_out_lab_mono)。最后计算两个 loss_mono 和 loss_cd 然后把它们加权求和起来得到 loss_final。</p>
<h3 id="forward"><a href="#forward" class="headerlink" title="forward"></a>forward</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[forward]</span><br><span class="line">forward_out &#x3D; out_dnn1</span><br><span class="line">normalize_posteriors &#x3D; True</span><br><span class="line">normalize_with_counts_from &#x3D; lab_cd</span><br><span class="line">save_out_file &#x3D; False</span><br><span class="line">require_decoding &#x3D; True</span><br></pre></td></tr></table></figure>
<p>forward 定义 forward 过程的参数，首先通过 forward_out 指定输出是 out_dnn1，也就是 softmax 分类概率的输出。normalize_posteriors 为 True 说明要把后验概率归一化成似然概率。normalize_with_counts_from 指定 lab_cd，这是在前面的 dataset3 里定义的 lab_name。</p>
<p>save_out_file 为 False 说明后验概率文件不用时会删掉，如果调试的话可以设置为 True。require_decoding 指定是否需要对输出进行解码，我们这里是需要的。</p>
<h3 id="decoding"><a href="#decoding" class="headerlink" title="decoding"></a>decoding</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[decoding]</span><br><span class="line">decoding_script_folder &#x3D; kaldi_decoding_scripts&#x2F;</span><br><span class="line">decoding_script &#x3D; decode_dnn.sh</span><br><span class="line">decoding_proto &#x3D; proto&#x2F;decoding.proto</span><br><span class="line">min_active &#x3D; 200</span><br><span class="line">max_active &#x3D; 7000</span><br><span class="line">max_mem &#x3D; 50000000</span><br><span class="line">beam &#x3D; 13.0</span><br><span class="line">latbeam &#x3D; 8.0</span><br><span class="line">acwt &#x3D; 0.2</span><br><span class="line">max_arcs &#x3D; -1</span><br><span class="line">skip_scoring &#x3D; false</span><br><span class="line">scoring_script &#x3D; local&#x2F;score.sh</span><br><span class="line">scoring_opts &#x3D; &quot;--min-lmwt 1 --max-lmwt 10&quot;</span><br><span class="line">norm_vars &#x3D; False</span><br></pre></td></tr></table></figure>
<p>decoding 设置解码器的参数，我们这里就不解释了，读者可以参考 Kaldi的文档或者本书前面介绍的相关内容。</p>
<h3 id="块配置文件"><a href="#块配置文件" class="headerlink" title="块配置文件"></a>块配置文件</h3><p>块配置文件和全局配置文件非常类似，它是 run_nn 在训练一个块的数据时的配置，它有一个配置 to_do={train, valid, forward}，用来说明当前的配置是训练、验证还是 forward(测试)。</p>
<hr>
<h2 id="自己用PyTorch实现神经网络-声学模型"><a href="#自己用PyTorch实现神经网络-声学模型" class="headerlink" title="自己用PyTorch实现神经网络(声学模型)"></a>自己用PyTorch实现神经网络(声学模型)</h2><p>我们可以参考 neural_networks.py 的 MLP 实现自己的网络模型。</p>
<h3 id="创建proto文件"><a href="#创建proto文件" class="headerlink" title="创建proto文件"></a>创建proto文件</h3><p>比如创建 proto/myDNN.proto，在这里定义模型的超参数。我们可以参考 MLP.proto，它的内容如下(前面介绍过了)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[proto]</span><br><span class="line">dnn_lay&#x3D;str_list</span><br><span class="line">dnn_drop&#x3D;float_list(0.0,1.0)</span><br><span class="line">dnn_use_laynorm_inp&#x3D;bool</span><br><span class="line">dnn_use_batchnorm_inp&#x3D;bool</span><br><span class="line">dnn_use_batchnorm&#x3D;bool_list</span><br><span class="line">dnn_use_laynorm&#x3D;bool_list</span><br><span class="line">dnn_act&#x3D;str_list</span><br></pre></td></tr></table></figure><br>dnn_lay 是一个字符串的 list，用逗号分开，比如我们前面的配置：dnn_lay = 1024,1024,1024,1024,N_out_lab_cd。其余的类似。bool 表示取值只能是 True 或者 False。float_list(0.0,1.0) 表示这是一个浮点数的 list，并且每一个值的范围都是必须在 (0, 1) 之间。</p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>我们可以参考 neural_networks.py 的 MLP 类。我们需要实现 <code>__init__</code> 和 forward 两个方法。<code>__init__</code> 有两个参数：options 表示参数，也就是 PyTorch-Kaldi 自动从前面的配置文件里提取的参数，比如 dnn_lay 等；另一个参数是 inp_dim，表示输入的大小(不包含 batch 维)。</p>
<p>我们下面来简单的看一下MLP是怎么实现的。</p>
<h4 id="init"><a href="#init" class="headerlink" title="__init__"></a>__init__</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, options,inp_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.input_dim=inp_dim</span><br><span class="line">        self.dnn_lay=<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, options[<span class="string">&#x27;dnn_lay&#x27;</span>].split(<span class="string">&#x27;,&#x27;</span>)))</span><br><span class="line">        self.dnn_drop=<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">float</span>, options[<span class="string">&#x27;dnn_drop&#x27;</span>].split(<span class="string">&#x27;,&#x27;</span>))) </span><br><span class="line">        self.dnn_use_batchnorm=<span class="built_in">list</span>(<span class="built_in">map</span>(strtobool, options[<span class="string">&#x27;dnn_use_batchnorm&#x27;</span>].split(<span class="string">&#x27;,&#x27;</span>)))</span><br><span class="line">        self.dnn_use_laynorm=<span class="built_in">list</span>(<span class="built_in">map</span>(strtobool, options[<span class="string">&#x27;dnn_use_laynorm&#x27;</span>].split(<span class="string">&#x27;,&#x27;</span>))) </span><br><span class="line">        self.dnn_use_laynorm_inp=strtobool(options[<span class="string">&#x27;dnn_use_laynorm_inp&#x27;</span>])</span><br><span class="line">        self.dnn_use_batchnorm_inp=strtobool(options[<span class="string">&#x27;dnn_use_batchnorm_inp&#x27;</span>])</span><br><span class="line">        self.dnn_act=options[<span class="string">&#x27;dnn_act&#x27;</span>].split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">       </span><br><span class="line">        self.wx  = nn.ModuleList([])</span><br><span class="line">        self.bn  = nn.ModuleList([])</span><br><span class="line">        self.ln  = nn.ModuleList([])</span><br><span class="line">        self.act = nn.ModuleList([])</span><br><span class="line">        self.drop = nn.ModuleList([])</span><br><span class="line">       </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># input layer normalization</span></span><br><span class="line">        <span class="keyword">if</span> self.dnn_use_laynorm_inp:</span><br><span class="line">           self.ln0=LayerNorm(self.input_dim)</span><br><span class="line">          </span><br><span class="line">        <span class="comment"># input batch normalization    </span></span><br><span class="line">        <span class="keyword">if</span> self.dnn_use_batchnorm_inp:</span><br><span class="line">           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=<span class="number">0.05</span>)</span><br><span class="line">           </span><br><span class="line">           </span><br><span class="line">        self.N_dnn_lay=<span class="built_in">len</span>(self.dnn_lay)</span><br><span class="line">             </span><br><span class="line">        current_input=self.input_dim</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialization of hidden layers</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.N_dnn_lay):</span><br><span class="line">            </span><br><span class="line">             <span class="comment"># dropout</span></span><br><span class="line">             self.drop.append(nn.Dropout(p=self.dnn_drop[i]))</span><br><span class="line">             </span><br><span class="line">             <span class="comment"># activation</span></span><br><span class="line">             self.act.append(act_fun(self.dnn_act[i]))</span><br><span class="line">             </span><br><span class="line">             </span><br><span class="line">             add_bias=<span class="literal">True</span></span><br><span class="line">             </span><br><span class="line">             <span class="comment"># layer norm initialization</span></span><br><span class="line">             self.ln.append(LayerNorm(self.dnn_lay[i]))</span><br><span class="line">             self.bn.append(nn.BatchNorm1d(self.dnn_lay[i],momentum=<span class="number">0.05</span>))</span><br><span class="line">             </span><br><span class="line">             <span class="keyword">if</span> self.dnn_use_laynorm[i] <span class="keyword">or</span> self.dnn_use_batchnorm[i]:</span><br><span class="line">                 add_bias=<span class="literal">False</span></span><br><span class="line">             </span><br><span class="line">                  </span><br><span class="line">             <span class="comment"># Linear operations</span></span><br><span class="line">             self.wx.append(nn.Linear(current_input, self.dnn_lay[i],bias=add_bias))</span><br><span class="line">             </span><br><span class="line">             <span class="comment"># weight initialization</span></span><br><span class="line">             self.wx[i].weight = torch.nn.Parameter(torch.Tensor(self.dnn_lay[i],current_input).</span><br><span class="line">		uniform_(-np.sqrt(<span class="number">0.01</span>/(current_input+self.dnn_lay[i])),</span><br><span class="line">			np.sqrt(<span class="number">0.01</span>/(current_input+self.dnn_lay[i]))))</span><br><span class="line">             self.wx[i].bias = torch.nn.Parameter(torch.zeros(self.dnn_lay[i]))</span><br><span class="line">             </span><br><span class="line">             current_input=self.dnn_lay[i]</span><br><span class="line">             </span><br><span class="line">        self.out_dim=current_input</span><br></pre></td></tr></table></figure>
<p>代码很长，但是其实很简单，首先从 options 里提取一些参数，比如<code>self.dnn_lay=list(map(int, options[&#39;dnn_lay&#39;].split(&#39;,&#39;)))</code>，就可以知道每一层的大小。</p>
<p>然后是根据每一层的配置分别构造线性层、BatchNorm、LayerNorm、激活函数和 Dropout，保存到 <code>self.wx</code>、<code>self.bn</code>、<code>self.ln</code>、<code>self.act</code> 和 <code>self.drop</code>这 5 个 <code>nn.ModuleList</code>里。</p>
<h4 id="forward-1"><a href="#forward-1" class="headerlink" title="forward"></a>forward</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    </span><br><span class="line">  <span class="comment"># Applying Layer/Batch Norm</span></span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">bool</span>(self.dnn_use_laynorm_inp):</span><br><span class="line">    x=self.ln0((x))</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">bool</span>(self.dnn_use_batchnorm_inp):</span><br><span class="line"></span><br><span class="line">    x=self.bn0((x))</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.N_dnn_lay):</span><br><span class="line">       </span><br><span class="line">      <span class="keyword">if</span> self.dnn_use_laynorm[i] <span class="keyword">and</span> <span class="keyword">not</span>(self.dnn_use_batchnorm[i]):</span><br><span class="line">       x = self.drop[i](self.act[i](self.ln[i](self.wx[i](x))))</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">if</span> self.dnn_use_batchnorm[i] <span class="keyword">and</span> <span class="keyword">not</span>(self.dnn_use_laynorm[i]):</span><br><span class="line">       x = self.drop[i](self.act[i](self.bn[i](self.wx[i](x))))</span><br><span class="line">       </span><br><span class="line">      <span class="keyword">if</span> self.dnn_use_batchnorm[i]==<span class="literal">True</span> <span class="keyword">and</span> self.dnn_use_laynorm[i]==<span class="literal">True</span>:</span><br><span class="line">       x = self.drop[i](self.act[i](self.bn[i](self.ln[i](self.wx[i](x)))))</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">if</span> self.dnn_use_batchnorm[i]==<span class="literal">False</span> <span class="keyword">and</span> self.dnn_use_laynorm[i]==<span class="literal">False</span>:</span><br><span class="line">       x = self.drop[i](self.act[i](self.wx[i](x)))</span><br><span class="line">        </span><br><span class="line">      </span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>forward 就用前面定义的 Module 来计算，代码非常简单。不熟悉 PyTorch 的读者可以参考官方文档。</p>
<h3 id="在配置文件里使用我们自定义的网络"><a href="#在配置文件里使用我们自定义的网络" class="headerlink" title="在配置文件里使用我们自定义的网络"></a>在配置文件里使用我们自定义的网络</h3><p>我们这里假设 myDNN 的实现和 MLP 完全一样，那么配置也是类似的，我们可以基于 <code>cfg/TIMIT_baselines/TIMIT_MLP_mfcc_basic.cfg</code> 进行简单的修改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[architecture1]</span><br><span class="line">arch_name&#x3D; mynetwork</span><br><span class="line">arch_library&#x3D;neural_networks # 假设myDNN类也放在neural_networks.py里</span><br><span class="line">arch_class&#x3D;myDNN </span><br><span class="line">arch_seq_model&#x3D;False # 我们的模型是非序列的</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># 下面的配置和MLP完全一样，如果我们实现的网络有不同的结构或者超参数，那么我们应该知道怎么设置它们</span><br><span class="line">dnn_lay&#x3D;1024,1024,1024,1024,1024,N_out_lab_cd</span><br><span class="line">dnn_drop&#x3D;0.15,0.15,0.15,0.15,0.15,0.0</span><br><span class="line">dnn_use_laynorm_inp&#x3D;False</span><br><span class="line">dnn_use_batchnorm_inp&#x3D;False</span><br><span class="line">dnn_use_batchnorm&#x3D;True,True,True,True,True,False</span><br><span class="line">dnn_use_laynorm&#x3D;False,False,False,False,False,False</span><br><span class="line">dnn_act&#x3D;relu,relu,relu,relu,relu,softmax</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其余的配置都不变就行了，我们把这个文件另存为 <code>cfg/myDNN_exp.cfg</code>。</p>
<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run_exp.sh cfg/myDNN_exp.cfg</span><br></pre></td></tr></table></figure>
<p>如果出现问题，我们首先可以去查看 <code>log.log</code> 的错误信息。</p>
<hr>
<h2 id="超参数搜索"><a href="#超参数搜索" class="headerlink" title="超参数搜索"></a>超参数搜索</h2><p>我们通常需要尝试很多种超参数的组合来获得最好的模型，一种常见的超参数搜索方法就是随机搜索。我们当然可以自己设置各种超参数的组合，但是这比较麻烦，PyTorch-Kaldi 提供工具随机自动生成不同超参数的配置文件，<code>tune_hyperparameters.py</code> 就是用于这个目的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python tune_hyperparameters.py cfg/TIMIT_MLP_mfcc.cfg exp/TIMIT_MLP_mfcc_tuning 10 arch_lr=randfloat(0.001,0.01) batch_size_train=randint(32,256) dnn_act=choose_str&#123;relu,relu,relu,relu,softmax|tanh,tanh,tanh,tanh,softmax&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>第一个参数 <code>cfg/TIMIT_MLP_mfcc.cfg</code> 是一个参考的”模板”配置，而第二个参数 <code>exp/TIMIT_MLP_mfcc_tuning</code> 是一个目录，用于存放生成的配置文件。</p>
<p>第三个参数 10 表示需要生成 10 个配置文件。后面的参数说明随机哪些配置项。</p>
<p>比如 <code>arch_lr=randfloat(0.001,0.01)</code> 表示 learning rate 用 (0.001, 0.01) 直接均匀分布的随机数产生。</p>
<p><code>dnn_act=choose_str&#123;relu,relu,relu,relu,softmax\|tanh,tanh,tanh,tanh,softmax&#125;</code> 表示激活函数从”relu,relu,relu,relu,softmax”和”tanh,tanh,tanh,tanh,softmax”里随机选择。</p>
<hr>
<h2 id="使用自己的数据集"><a href="#使用自己的数据集" class="headerlink" title="使用自己的数据集"></a>使用自己的数据集</h2><p>使用自己的数据集可以参考前面的TIMIT或者LibriSpeech示例，我们通常需要如下步骤：</p>
<ul>
<li><p>准备Kaldi脚本，请参考Kaldi官方文档。</p>
</li>
<li><p>使用Kaldi对训练、验证和测试数据做强制对齐。</p>
</li>
<li><p>创建一个 PyTorch-Kaldi 的配置文件 cfg_file</p>
</li>
<li><p>训练 python run_exp.sh $cfg_file</p>
</li>
</ul>
<hr>
<h2 id="使用自定义的特征"><a href="#使用自定义的特征" class="headerlink" title="使用自定义的特征"></a>使用自定义的特征</h2><p>PyTorch-Kaldi 支持 Kaldi 的 ark 格式的特征文件，如果想加入自己的特征，需要保存为 ark 格式。读者可以参考 <a target="_blank" rel="noopener" href="https://github.com/vesis84/kaldi-io-for-python">kaldi-io-for-python</a> 来实现怎么把 numpy(特征当然就是一些向量了) 转换成 ark 格式的特征文件。也可以参考 <code>save_raw_fea.py</code>，这个脚本把原始的特征转换成 ark 格式，然后用于后续的神经网络训练。</p>
<h2 id="Batch大小、learning-rate和dropout的调度"><a href="#Batch大小、learning-rate和dropout的调度" class="headerlink" title="Batch大小、learning rate和dropout的调度"></a>Batch大小、learning rate和dropout的调度</h2><p>我们通常需要根据训练的进度动态的调整learning rate等超参数，PyTorch-Kaldi最新版本提供了灵活方便的配置方式，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_size_train &#x3D; 128*12 | 64*10 | 32*2</span><br></pre></td></tr></table></figure>
<p>上面配置的意思是训练的时候前12个epoch使用128的batch，然后10个epoch使用大小64的batch，最后两个epoch的batch大小是32。</p>
<p>类似的，我们可以定义 learning rate：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arch_lr &#x3D; 0.08*10|0.04*5|0.02*3|0.01*2|0.005*2|0.0025*2</span><br></pre></td></tr></table></figure><br>它表示前10个epoch的learning rate是0.08，接下来的5个epoch是0.04，然后用0.02训练3个epoch，……。</p>
<p>dnn的dropout可以如下的方式表示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dnn_drop &#x3D; 0.15*12|0.20*12,0.15,0.15*10|0.20*14,0.15,0.0</span><br></pre></td></tr></table></figure>
<p>这是用逗号分开配置的 5 个全连接层的 dropout，对于第一层来说，前 12 个 epoch 的 dropout 是 0.15 后 12 个是 0.20。第二层的 dropout 一直是 0.15。第三层的前 10 个 epoch 的 dropout 是 0.15 后 14 个 epoch 是 0.20，……。</p>
<hr>
<h2 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h2><p>目前 PyTorch-Kaldi 最大的问题无法实现 online 的 Decoder，因此只能做 offline 的语音识别。具体细节感兴趣的读者请参考<a target="_blank" rel="noopener" href="https://github.com/mravanelli/pytorch-kaldi/issues/56">这个ISSUE</a>，可能在未来的版本里会增加online decoding的支持。</p>

        </div>

        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tag/PyTorch/"># PyTorch</a>
                    
                        <a href="/tag/Kaldi/"># Kaldi</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/12/hexo-theme-chic/">Chic Theme Doc</a>
            
            
            <a class="next" rel="next" href="/2020/01/bye-2019/">除夕清灰</a>
            
        </section>


    </article>
</div>


    <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.2/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.5.2/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: '7ac4af6b2ef79db6191b',
        clientSecret: 'b5cf578803e18f2c9c3203761db3988e9073361d',
        repo: 'Baileyswu.github.io',
        owner: 'Baileyswu',
        admin: 'Baileyswu',
        id: location.pathname,
        labels: 'comments'.split(',').filter(l => l),
        perPage: 15,
        pagerDirection: 'first',
        createIssueManually: true,
        distractionFreeMode: false
      })
      gitalk.render('gitalk-container')
</script>


    <div id="valine-container"></div>
    <div id="valine_container" class="valine_thread"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
    var GUEST_INFO = ['nick','mail','link'];
    var guest_info = 'nick,mail,link'.split(',').filter(function(item){
        return GUEST_INFO.indexOf(item) > -1
    });
    var valine = new Valine();
    valine.init({
        el: '#valine_container',
        appId: "0Fj98eQg9XCdRJPI2p1yxYCN-gzGzoHsz",
        appKey: "Qsrhj224GhWhBgFDfnfmuRCD",
        placeholder: "Listen to Me",
        pageSize: '10',
        avatar: 'https://avatars3.githubusercontent.com/u/13285397?s=460&amp;v=4',
        lang: 'zh-cn',
        visitor: true
    })
</script>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Baileyswu | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
